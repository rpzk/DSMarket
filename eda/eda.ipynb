{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar variáveis de ambiente\n",
    "load_dotenv()\n",
    "\n",
    "# Obter variáveis de ambiente\n",
    "user = os.environ.get('DB_USER')\n",
    "password = os.environ.get('DB_PASSWORD')\n",
    "host = os.environ.get('DB_HOST')\n",
    "port = os.environ.get('DB_PORT', '5432')\n",
    "database = os.environ.get('DB_NAME')\n",
    "\n",
    "# Criar string de conexão\n",
    "connection_string = f'postgresql://{user}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Criar conexão com o banco de dados\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_in_chunks(query, engine, chunksize=50000):\n",
    "    # Lista para armazenar os DataFrames Dask\n",
    "    df_list = []\n",
    "    \n",
    "    # Ler os dados em chunks usando pandas\n",
    "    for chunk in pd.read_sql(query, engine, chunksize=chunksize):\n",
    "        # Converter o chunk pandas DataFrame para Dask DataFrame\n",
    "        ddf_chunk = dd.from_pandas(chunk, npartitions=1)\n",
    "        df_list.append(ddf_chunk)\n",
    "    \n",
    "    # Concatenar todos os Dask DataFrames em um único DataFrame Dask\n",
    "    df = dd.concat(df_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT * FROM ds_market'\n",
    "df = load_data_in_chunks(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por 'item' e somar as vendas\n",
    "item_sales = df.groupby('item')['sales'].sum().compute()\n",
    "\n",
    "# Converter para um DataFrame pandas para facilitar a manipulação\n",
    "item_sales = item_sales.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itens mais vendidos:\n",
      "                   item    sales\n",
      "1482  SUPERMARKET_3_090  1004721\n",
      "1501  SUPERMARKET_3_586   920242\n",
      "2966  SUPERMARKET_3_252   565299\n",
      "2162  SUPERMARKET_3_555   491287\n",
      "2933  SUPERMARKET_3_714   396172\n",
      "2466  SUPERMARKET_3_587   396119\n",
      "568   SUPERMARKET_3_694   390001\n",
      "2928  SUPERMARKET_3_226   363082\n",
      "842   SUPERMARKET_3_202   295983\n",
      "3046  SUPERMARKET_3_723   284333\n"
     ]
    }
   ],
   "source": [
    "# Ordenar em ordem decrescente de vendas\n",
    "top_items = item_sales.sort_values('sales', ascending=False)\n",
    "\n",
    "# Exibir os top 10 itens mais vendidos\n",
    "print(\"Itens mais vendidos:\")\n",
    "print(top_items.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itens menos vendidos:\n",
      "                     item  sales\n",
      "2416  HOME_&_GARDEN_2_101    568\n",
      "1253     ACCESORIES_2_119    660\n",
      "1885  HOME_&_GARDEN_2_175    718\n",
      "3025     ACCESORIES_2_084    746\n",
      "2178  HOME_&_GARDEN_2_005    757\n",
      "2775     ACCESORIES_2_111    770\n",
      "1838  HOME_&_GARDEN_2_245    780\n",
      "2199  HOME_&_GARDEN_2_130    789\n",
      "171   HOME_&_GARDEN_2_307    796\n",
      "66       ACCESORIES_2_023    800\n"
     ]
    }
   ],
   "source": [
    "# Ordenar em ordem crescente de vendas\n",
    "bottom_items = item_sales.sort_values('sales', ascending=True)\n",
    "\n",
    "# Exibir os top 10 itens menos vendidos\n",
    "print(\"Itens menos vendidos:\")\n",
    "print(bottom_items.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lojas com mais vendas:\n",
      "               store     sales\n",
      "9            Tribeca  11192410\n",
      "0  Greenwich_Village   7701312\n",
      "4            Roxbury   7218243\n",
      "6           Yorktown   6548196\n",
      "3      Queen_Village   6432215\n",
      "1           Back_Bay   6091767\n",
      "2             Harlem   5688194\n",
      "8          South_End   5597629\n",
      "5    Midtown_Village   5152300\n",
      "7           Brooklyn   4105128\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por 'store' e somar as vendas\n",
    "store_sales = df.groupby('store')['sales'].sum().compute().reset_index()\n",
    "\n",
    "# Ordenar em ordem decrescente de vendas\n",
    "top_stores = store_sales.sort_values('sales', ascending=False)\n",
    "\n",
    "# Exibir as top 10 lojas com mais vendas\n",
    "print(\"Lojas com mais vendas:\")\n",
    "print(top_stores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regiões com mais vendas:\n",
      "         region     sales\n",
      "0      New York  28687044\n",
      "2        Boston  18907639\n",
      "1  Philadelphia  18132711\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por 'region' e somar as vendas\n",
    "region_sales = df.groupby('region')['sales'].sum().compute().reset_index()\n",
    "\n",
    "# Ordenar em ordem decrescente de vendas\n",
    "top_regions = region_sales.sort_values('sales', ascending=False)\n",
    "\n",
    "# Exibir as regiões com mais vendas\n",
    "print(\"Regiões com mais vendas:\")\n",
    "print(top_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorias com mais vendas:\n",
      "        category     sales\n",
      "2    SUPERMARKET  45114388\n",
      "0  HOME_&_GARDEN  14486027\n",
      "1     ACCESORIES   6126979\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por 'category' e somar as vendas\n",
    "category_sales = df.groupby('category')['sales'].sum().compute().reset_index()\n",
    "\n",
    "# Ordenar em ordem decrescente de vendas\n",
    "top_categories = category_sales.sort_values('sales', ascending=False)\n",
    "\n",
    "# Exibir as categorias com mais vendas\n",
    "print(\"Categorias com mais vendas:\")\n",
    "print(top_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter a coluna 'date' para datetime\n",
    "df['date'] = dd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma coluna indicando se é um evento\n",
    "df['is_event'] = df['event'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coletar as datas dos eventos\n",
    "event_dates = df[df['is_event']].date.compute().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rpiaz\\Desenvolvimento\\DSMarket\\.venv\\Lib\\site-packages\\dask_expr\\_collection.py:4376: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('date', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Converter as datas dos eventos para um conjunto para busca eficiente\n",
    "event_dates_set = set(event_dates.dt.date)\n",
    "\n",
    "# Criar uma coluna 'event_period' categorizando cada data\n",
    "def get_event_period(date):\n",
    "    if date.date() in event_dates_set:\n",
    "        return 'Durante o Evento'\n",
    "    elif any((date.date() - pd.Timedelta(days=i)) in event_dates_set for i in range(1, 8)):\n",
    "        return 'Antes do Evento'\n",
    "    elif any((date.date() + pd.Timedelta(days=i)) in event_dates_set for i in range(1, 8)):\n",
    "        return 'Após o Evento'\n",
    "    else:\n",
    "        return 'Fora do Evento'\n",
    "\n",
    "df['event_period'] = df['date'].apply(get_event_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vendas por período de evento:\n",
      "Dask DataFrame Structure:\n",
      "              event_period  sales\n",
      "npartitions=1                    \n",
      "                    object  int64\n",
      "                       ...    ...\n",
      "Dask Name: reset_index, 1181 expressions\n",
      "Expr=ResetIndex(frame=Sum(frame=(Assign(frame=Assign(frame=Assign(frame=Concat(frames=[df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df, df], )))))[['sales', 'event_period']], observed=False, chunk_kwargs={'numeric_only': False}, aggregate_kwargs={'numeric_only': False}, _slice='sales'))\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por período de evento e somar as vendas\n",
    "event_sales = df.groupby('event_period')['sales'].sum().reset_index()\n",
    "\n",
    "print(\"Vendas por período de evento:\")\n",
    "print(event_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar os períodos para exibição lógica\n",
    "event_sales['event_period'] = pd.Categorical(\n",
    "    event_sales['event_period'], \n",
    "    categories=['Antes do Evento', 'Durante o Evento', 'Após o Evento', 'Fora do Evento'], \n",
    "    ordered=True\n",
    ").astype(str)\n",
    "\n",
    "# Ensure event_sales is a Pandas DataFrame\n",
    "if not isinstance(event_sales, pd.DataFrame):\n",
    "    raise TypeError(\"event_sales must be a Pandas DataFrame\")\n",
    "\n",
    "# Convert the entire DataFrame to Dask DataFrame\n",
    "event_sales = dd.from_pandas(event_sales, npartitions=1)\n",
    "\n",
    "# Sort the Dask DataFrame\n",
    "event_sales = event_sales.compute().sort_values('event_period')\n",
    "\n",
    "# Criar um gráfico de barras\n",
    "plt.bar(event_sales['event_period'], event_sales['sales'])\n",
    "plt.xlabel('Período em Relação ao Evento')\n",
    "plt.ylabel('Vendas Totais')\n",
    "plt.title('Impacto de Eventos nas Vendas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar as vendas por data\n",
    "daily_sales = df.groupby('date')['sales'].sum().compute().reset_index()\n",
    "\n",
    "# Visualizar as vendas diárias\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(daily_sales['date'], daily_sales['sales'])\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Vendas Totais')\n",
    "plt.title('Vendas Diárias')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar coluna com o número da semana\n",
    "daily_sales['week'] = daily_sales['date'].dt.isocalendar().week\n",
    "\n",
    "# Agrupar as vendas por semana\n",
    "weekly_sales = daily_sales.groupby('week')['sales'].sum().reset_index()\n",
    "\n",
    "# Visualizar as vendas semanais\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(weekly_sales['week'], weekly_sales['sales'])\n",
    "plt.xlabel('Semana do Ano')\n",
    "plt.ylabel('Vendas Totais')\n",
    "plt.title('Vendas Semanais')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' is in datetime format\n",
    "df['date'] = dd.to_datetime(df['date'])\n",
    "\n",
    "# Set 'date' as the index\n",
    "df = df.set_index('date')\n",
    "\n",
    "# Resample sales data to get daily total sales\n",
    "daily_sales = df['sales'].resample('D').sum().compute()\n",
    "\n",
    "# Plot daily sales\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(daily_sales.index, daily_sales.values)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Daily Total Sales Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of top 5 items\n",
    "top_5_items = top_items['item'].head(5).tolist()\n",
    "\n",
    "# Filter data for top 5 items\n",
    "df_top_items = df[df['item'].isin(top_5_items)]\n",
    "\n",
    "# Group by date and item, then sum sales\n",
    "item_daily_sales = df_top_items.groupby(['date', 'item'])['sales'].sum().compute().reset_index()\n",
    "\n",
    "# Pivot the data for plotting\n",
    "item_daily_sales_pivot = item_daily_sales.pivot(index='date', columns='item', values='sales')\n",
    "\n",
    "# Plot sales trends for top 5 items\n",
    "plt.figure(figsize=(14,7))\n",
    "for item in top_5_items:\n",
    "    plt.plot(item_daily_sales_pivot.index, item_daily_sales_pivot[item], label=item)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Daily Sales Trends for Top 5 Items')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Since statsmodels requires a pandas Series, ensure the data is in the correct format\n",
    "daily_sales = daily_sales.asfreq('D')  # Ensure the index is a DateTimeIndex with daily frequency\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "decomposition = seasonal_decompose(daily_sales, model='additive')\n",
    "\n",
    "# Plot the decomposition\n",
    "decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average sell_price per item per day\n",
    "avg_price = df.groupby(['date', 'item'])['sell_price'].mean().compute().reset_index()\n",
    "\n",
    "# Compute total sales per item per day\n",
    "total_sales = df.groupby(['date', 'item'])['sales'].sum().compute().reset_index()\n",
    "\n",
    "# Merge the dataframes\n",
    "price_sales = pd.merge(total_sales, avg_price, on=['date', 'item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, analyze one item\n",
    "item_to_analyze = top_items['item'].iloc[0]\n",
    "\n",
    "item_data = price_sales[price_sales['item'] == item_to_analyze]\n",
    "\n",
    "# Calculate percentage change in price and sales\n",
    "item_data['price_pct_change'] = item_data['sell_price'].pct_change()\n",
    "item_data['sales_pct_change'] = item_data['sales'].pct_change()\n",
    "\n",
    "# Remove rows with NaN values\n",
    "item_data = item_data.dropna()\n",
    "\n",
    "# Calculate elasticity\n",
    "item_data['elasticity'] = item_data['sales_pct_change'] / item_data['price_pct_change']\n",
    "\n",
    "# Plot elasticity over time\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(item_data['date'], item_data['elasticity'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price Elasticity')\n",
    "plt.title(f'Price Elasticity Over Time for {item_to_analyze}')\n",
    "plt.show()\n",
    "\n",
    "# Average elasticity\n",
    "average_elasticity = item_data['elasticity'].mean()\n",
    "print(f\"Average price elasticity for {item_to_analyze}: {average_elasticity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute daily total sales\n",
    "daily_sales_df = df.groupby('date')['sales'].sum().compute().reset_index()\n",
    "\n",
    "# Create date features\n",
    "daily_sales_df['day_of_week'] = daily_sales_df['date'].dt.dayofweek\n",
    "daily_sales_df['month'] = daily_sales_df['date'].dt.month\n",
    "\n",
    "# Create lag features\n",
    "daily_sales_df['lag_1'] = daily_sales_df['sales'].shift(1)\n",
    "daily_sales_df['lag_7'] = daily_sales_df['sales'].shift(7)\n",
    "\n",
    "# Create rolling features\n",
    "daily_sales_df['rolling_mean_7'] = daily_sales_df['sales'].rolling(window=7).mean()\n",
    "daily_sales_df['rolling_std_7'] = daily_sales_df['sales'].rolling(window=7).std()\n",
    "\n",
    "# Create event indicator\n",
    "daily_sales_df['is_event'] = daily_sales_df['date'].isin(event_dates.compute()).astype(int)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "daily_sales_df = daily_sales_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target\n",
    "X = daily_sales_df[['day_of_week', 'month', 'lag_1', 'lag_7', 'rolling_mean_7', 'rolling_std_7', 'is_event']]\n",
    "y = daily_sales_df['sales']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(daily_sales_df['date'].iloc[-len(y_test):], y_test, label='Actual Sales')\n",
    "plt.plot(daily_sales_df['date'].iloc[-len(y_test):], y_pred, label='Predicted Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Actual vs Predicted Sales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute z-scores\n",
    "daily_sales_df['sales_zscore'] = (daily_sales_df['sales'] - daily_sales_df['sales'].mean()) / daily_sales_df['sales'].std()\n",
    "\n",
    "# Identify anomalies\n",
    "threshold = 3  # Adjust based on your needs\n",
    "anomalies = daily_sales_df[abs(daily_sales_df['sales_zscore']) > threshold]\n",
    "\n",
    "print(\"Anomalous sales days:\")\n",
    "print(anomalies[['date', 'sales', 'sales_zscore']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df has a 'customer_id' column\n",
    "\n",
    "# Compute Recency, Frequency, Monetary value\n",
    "import datetime as dt\n",
    "\n",
    "# Set analysis date as the last date in the dataset\n",
    "analysis_date = df['date'].max().compute()\n",
    "\n",
    "# Compute RFM metrics\n",
    "rfm = df.groupby('customer_id').agg({\n",
    "    'date': lambda x: (analysis_date - x.max()).days,\n",
    "    'id': 'count',\n",
    "    'sales': 'sum'\n",
    "}).compute()\n",
    "\n",
    "rfm.columns = ['Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# Assign scores\n",
    "rfm['R_Score'] = pd.qcut(rfm['Recency'], 5, labels=range(5, 0, -1))\n",
    "rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=range(1, 6))\n",
    "rfm['M_Score'] = pd.qcut(rfm['Monetary'], 5, labels=range(1, 6))\n",
    "\n",
    "# Compute RFM Score\n",
    "rfm['RFM_Score'] = rfm['R_Score'].astype(int) + rfm['F_Score'].astype(int) + rfm['M_Score'].astype(int)\n",
    "\n",
    "print(\"Customer Segmentation based on RFM:\")\n",
    "print(rfm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute daily sales per category\n",
    "category_daily_sales = df.groupby(['date', 'category'])['sales'].sum().compute().reset_index()\n",
    "\n",
    "# Pivot the data\n",
    "category_sales_pivot = category_daily_sales.pivot(index='date', columns='category', values='sales').fillna(0)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = category_sales_pivot.corr()\n",
    "\n",
    "print(\"Correlation between categories:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.layouts import column\n",
    "output_notebook()\n",
    "\n",
    "# Prepare data\n",
    "source = ColumnDataSource(data={\n",
    "    'date': daily_sales_df['date'],\n",
    "    'sales': daily_sales_df['sales']\n",
    "})\n",
    "\n",
    "# Create a figure\n",
    "p = figure(x_axis_type='datetime', title='Daily Sales', plot_height=350, plot_width=800)\n",
    "p.line('date', 'sales', source=source)\n",
    "p.xaxis.axis_label = 'Date'\n",
    "p.yaxis.axis_label = 'Sales'\n",
    "\n",
    "# Show the plot\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types to more efficient ones\n",
    "df['sales'] = df['sales'].astype('int32')\n",
    "df['sell_price'] = df['sell_price'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Only load data from the last 2 years\n",
    "query = \"SELECT * FROM ds_market WHERE date >= '2021-01-01'\"\n",
    "df = load_data_in_chunks(query, engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
