{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsão de Estoque para 28 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from google.cloud import exceptions\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Caminho para o arquivo de chave da conta de serviço\n",
    "service_account_path = 'tfm-sa.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "project_id = 'perseverance-332400'\n",
    "dataset_id = 'TFM'\n",
    "\n",
    "# Definir IDs de tabela separadas\n",
    "data_table_id = 'ds_market'               # Tabela com dados originais\n",
    "forecast_table_id = 'ds_market_forecast'  # Nova tabela para previsões\n",
    "\n",
    "full_data_table_id = f'{project_id}.{dataset_id}.{data_table_id}'\n",
    "\n",
    "# Função para configurar o cliente BigQuery usando credenciais específicas\n",
    "def initialize_bigquery_client():\n",
    "    return bigquery.Client(project=project_id, credentials=credentials)\n",
    "\n",
    "# Função para extrair dados históricos do BigQuery\n",
    "def get_historical_data_from_bigquery(query: str, client) -> pd.DataFrame:\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        data = query_job.to_dataframe()\n",
    "        logging.info(f\"Dados extraídos do BigQuery com {data.shape[0]} linhas e {data.shape[1]} colunas.\")\n",
    "        logging.info(f\"Colunas disponíveis nos dados extraídos: {data.columns.tolist()}\")\n",
    "        return data\n",
    "    except exceptions.GoogleCloudError as e:\n",
    "        logging.error(f\"Erro ao extrair dados do BigQuery: {e}\")\n",
    "        return pd.DataFrame()  # Retorna um DataFrame vazio para evitar falhas subsequentes\n",
    "\n",
    "# Função para preparar os dados\n",
    "def prepare_sales_data(raw_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    raw_data = raw_data.copy()\n",
    "    raw_data['date'] = pd.to_datetime(raw_data['date'])\n",
    "    raw_data['sales'] = raw_data['sales'].fillna(0)\n",
    "    sales_data = raw_data.groupby('date').agg({'sales': 'sum'}).sort_index()\n",
    "    if sales_data.isnull().any().any():\n",
    "        logging.warning(\"Existem datas sem dados de vendas.\")\n",
    "    sales_data = sales_data.asfreq('D')\n",
    "    sales_data['sales'] = sales_data['sales'].fillna(0)\n",
    "    logging.info(\"Dados de vendas preparados e limpos.\")\n",
    "    return sales_data\n",
    "\n",
    "# Função para treinar o modelo ARIMA usando pmdarima\n",
    "def train_forecast_model(data: pd.DataFrame):\n",
    "    logging.info(\"Treinando o modelo ARIMA com pmdarima...\")\n",
    "    series = data['sales']\n",
    "\n",
    "    try:\n",
    "        model = auto_arima(\n",
    "            series,\n",
    "            seasonal=False,\n",
    "            stepwise=True,\n",
    "            suppress_warnings=True,\n",
    "            error_action='ignore',\n",
    "            trace=True\n",
    "        )\n",
    "        order = model.order\n",
    "        logging.info(f\"Modelo treinado com sucesso com ordem {order}.\")\n",
    "        return model, order\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao ajustar o modelo ARIMA com pmdarima: {e}\")\n",
    "        raise\n",
    "\n",
    "# Função para gerar previsões\n",
    "def predict_sales(data: pd.DataFrame, model, steps=30, forecast_index=None) -> pd.DataFrame:\n",
    "    logging.info(f\"Gerando previsões para {steps} dias...\")\n",
    "    if model is None:\n",
    "        raise ValueError(\"Modelo inválido. Não é possível gerar previsões.\")\n",
    "    forecast = model.predict(n_periods=steps)\n",
    "    if forecast_index is not None:\n",
    "        forecast_df = pd.DataFrame({'date': forecast_index, 'forecast_sales': forecast})\n",
    "    else:\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'date': pd.date_range(data.index[-1] + timedelta(days=1), periods=steps),\n",
    "            'forecast_sales': forecast\n",
    "        })\n",
    "    logging.info(\"Previsão gerada com sucesso.\")\n",
    "    return forecast_df\n",
    "\n",
    "# Função para carregar previsões no BigQuery\n",
    "def store_forecast_results(data: pd.DataFrame, table_id: str, client):\n",
    "    logging.info(f\"Carregando previsões para a tabela {table_id} no BigQuery...\")\n",
    "    \n",
    "    # Remover coluna 'actual_sales' se ela não for necessária na tabela de previsões\n",
    "    if 'actual_sales' in data.columns:\n",
    "        data = data.drop(columns=['actual_sales'])\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\")  # Evita sobrescrever a tabela original\n",
    "    job = client.load_table_from_dataframe(data, table_id, job_config=job_config)\n",
    "    job.result()\n",
    "    logging.info(\"Previsões carregadas com sucesso no BigQuery.\")\n",
    "\n",
    "\n",
    "# Função para plotar previsões vs. dados reais\n",
    "def plot_forecast_with_actual(train_data: pd.DataFrame, test_data: pd.DataFrame, forecast: pd.DataFrame, store: str, item: str):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_data.index, train_data['sales'], label='Dados de Treinamento')\n",
    "    plt.plot(test_data.index, test_data['sales'], label='Vendas Reais (Teste)')\n",
    "    plt.plot(forecast['date'], forecast['forecast_sales'], label='Previsão', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(f'Previsão vs. Vendas Reais - Loja {store} Item {item}')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Vendas')\n",
    "    plt.show()\n",
    "\n",
    "# Pipeline completo para previsão de estoque dos itens por loja\n",
    "def run_forecast_pipeline_for_top_items_per_store(query: str, forecast_table_full_id: str):\n",
    "    client = initialize_bigquery_client()\n",
    "    raw_data = get_historical_data_from_bigquery(query, client)\n",
    "\n",
    "    # Verificar se as colunas necessárias existem\n",
    "    required_columns = {'store', 'item', 'date', 'sales'}\n",
    "    if not required_columns.issubset(raw_data.columns):\n",
    "        missing = required_columns - set(raw_data.columns)\n",
    "        raise ValueError(f\"As colunas a seguir estão faltando nos dados: {missing}\")\n",
    "\n",
    "    # Obter lista de lojas únicas\n",
    "    stores = raw_data['store'].unique()\n",
    "    all_forecasts = []\n",
    "\n",
    "    for store in stores:\n",
    "        logging.info(f\"\\nProcessando loja {store}...\")\n",
    "        store_data = raw_data[raw_data['store'] == store]\n",
    "        total_sales_per_item = store_data.groupby('item')['sales'].sum().reset_index()\n",
    "        top_items = total_sales_per_item.sort_values(by='sales', ascending=False).head(1)['item']  # número de itens a serem previstos\n",
    "\n",
    "        for item in top_items:\n",
    "            logging.info(f\"Processando item {item} na loja {store}...\")\n",
    "            item_data = store_data[store_data['item'] == item]\n",
    "            prepared_data = prepare_sales_data(item_data)\n",
    "\n",
    "            if len(prepared_data) < 2:\n",
    "                logging.warning(f\"Dados insuficientes para item {item} na loja {store}, pulando...\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Dividir os dados em treinamento e teste\n",
    "                train_data = prepared_data[prepared_data.index < '2015-12-01']\n",
    "                test_data = prepared_data[prepared_data.index >= '2015-12-01']\n",
    "\n",
    "                if len(train_data) < 2 or len(test_data) == 0:\n",
    "                    logging.warning(f\"Dados insuficientes para treinamento ou teste para item {item} na loja {store}, pulando...\")\n",
    "                    continue\n",
    "\n",
    "                model, order = train_forecast_model(train_data)\n",
    "                if model is None:\n",
    "                    logging.error(f\"Não foi possível ajustar o modelo para item {item} na loja {store}.\")\n",
    "                    continue\n",
    "\n",
    "                # Prever para o período de teste\n",
    "                steps = len(test_data)\n",
    "                forecast = predict_sales(train_data, model, steps=steps, forecast_index=test_data.index)\n",
    "                forecast['store'] = store\n",
    "                forecast['item'] = item\n",
    "\n",
    "                # Comparar previsões com vendas reais\n",
    "                forecast['actual_sales'] = test_data['sales'].values\n",
    "\n",
    "                # Calcular métricas de avaliação\n",
    "                rmse = mean_squared_error(forecast['actual_sales'], forecast['forecast_sales'], squared=False)\n",
    "                mae = mean_absolute_error(forecast['actual_sales'], forecast['forecast_sales'])\n",
    "                mape = np.mean(np.abs((forecast['actual_sales'] - forecast['forecast_sales']) / (forecast['actual_sales'] + 1e-5))) * 100  # Evitar divisão por zero\n",
    "\n",
    "                logging.info(f\"Loja {store}, Item {item} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, MAPE: {mape:.2f}%\")\n",
    "\n",
    "                # Adicionar informações ao DataFrame de previsões\n",
    "                forecast['rmse'] = rmse\n",
    "                forecast['mae'] = mae\n",
    "                forecast['mape'] = mape\n",
    "                forecast['order'] = str(order)\n",
    "\n",
    "                all_forecasts.append(forecast)\n",
    "\n",
    "                # Plotar previsões vs. vendas reais\n",
    "                plot_forecast_with_actual(train_data, test_data, forecast, store, item)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erro ao processar item {item} na loja {store}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if all_forecasts:\n",
    "        final_forecast = pd.concat(all_forecasts, ignore_index=True)\n",
    "        # Armazenar previsões na tabela de previsões\n",
    "        store_forecast_results(final_forecast, forecast_table_full_id, client)\n",
    "    else:\n",
    "        logging.warning(\"Nenhuma previsão foi gerada.\")\n",
    "\n",
    "# Definir a consulta SQL para carregar os dados de 2015\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{full_data_table_id}`\n",
    "WHERE EXTRACT(YEAR FROM date) = 2015\n",
    "\"\"\"\n",
    "\n",
    "# Executar o pipeline, especificando a tabela de previsões\n",
    "run_forecast_pipeline_for_top_items_per_store(\n",
    "    query,\n",
    "    f'{project_id}.{dataset_id}.{forecast_table_id}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from pmdarima.arima import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import logging\n",
    "\n",
    "# Configuração de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Função para verificar estacionariedade e aplicar diferenciação, se necessário\n",
    "def check_and_differentiate(data: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    result = adfuller(data[column])\n",
    "    logging.info(f\"ADF Statistic: {result[0]}, Valor-p: {result[1]}\")\n",
    "    if result[1] > 0.05:\n",
    "        logging.info(\"Série não estacionária. Aplicando diferenciação.\")\n",
    "        data[column] = data[column].diff().dropna()\n",
    "    else:\n",
    "        logging.info(\"Série já é estacionária.\")\n",
    "    return data\n",
    "\n",
    "# Função para normalizar os dados de vendas\n",
    "def normalize_sales_data(data: pd.DataFrame, column: str) -> tuple:\n",
    "    scaler = MinMaxScaler()\n",
    "    data[column] = scaler.fit_transform(data[[column]])\n",
    "    logging.info(\"Dados de vendas normalizados com MinMaxScaler.\")\n",
    "    return data, scaler\n",
    "\n",
    "# Função para otimizar o modelo ARIMA testando diferentes combinações de parâmetros\n",
    "def optimize_arima(data: pd.Series):\n",
    "    p = d = q = range(0, 3)\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "    best_aic = float(\"inf\")\n",
    "    best_order = None\n",
    "    best_model = None\n",
    "\n",
    "    for order in pdq:\n",
    "        try:\n",
    "            model = ARIMA(order=order)\n",
    "            model.fit(data)\n",
    "            aic = model.aic()\n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_order = order\n",
    "                best_model = model\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    logging.info(f\"Melhor modelo ARIMA: ordem {best_order} com AIC {best_aic}\")\n",
    "    return best_model\n",
    "\n",
    "# Função para validação cruzada em séries temporais\n",
    "def cross_validate_arima(data: pd.Series, model_func, splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=splits)\n",
    "    errors = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(data):\n",
    "        train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "        model = model_func(train)\n",
    "        forecast = model.predict(n_periods=len(test))\n",
    "        rmse = mean_squared_error(test, forecast, squared=False)\n",
    "        errors.append(rmse)\n",
    "\n",
    "    avg_rmse = np.mean(errors)\n",
    "    logging.info(f\"RMSE médio com validação cruzada: {avg_rmse}\")\n",
    "    return avg_rmse\n",
    "\n",
    "# Função para calcular as métricas de erro\n",
    "def calculate_metrics(actual, forecast):\n",
    "    rmse = mean_squared_error(actual, forecast, squared=False)\n",
    "    mae = mean_absolute_error(actual, forecast)\n",
    "    mape = np.mean(np.abs((actual - forecast) / (actual + 1e-5))) * 100  # evitar divisão por zero\n",
    "\n",
    "    logging.info(f\"RMSE: {rmse}, MAE: {mae}, MAPE: {mape}%\")\n",
    "    return rmse, mae, mape\n",
    "\n",
    "# Função principal para executar o pipeline de previsão\n",
    "def run_forecast_pipeline(data: pd.DataFrame, column: str):\n",
    "    # Verificar e diferenciar para estacionariedade\n",
    "    data = check_and_differentiate(data, column)\n",
    "\n",
    "    # Normalizar os dados\n",
    "    data, scaler = normalize_sales_data(data, column)\n",
    "\n",
    "    # Otimizar ARIMA e validar o modelo\n",
    "    model = optimize_arima(data[column])\n",
    "    rmse_cv = cross_validate_arima(data[column], lambda x: optimize_arima(x))\n",
    "\n",
    "    # Gerar previsão para os próximos 28 dias\n",
    "    forecast = model.predict(n_periods=28)\n",
    "    forecast = scaler.inverse_transform(forecast.values.reshape(-1, 1)).flatten()  # desfazendo a normalização\n",
    "\n",
    "    # Calcular métricas de erro usando os últimos 28 dias de dados reais\n",
    "    actual = scaler.inverse_transform(data[column].values[-28:].reshape(-1, 1)).flatten()\n",
    "    rmse, mae, mape = calculate_metrics(actual, forecast)\n",
    "\n",
    "    return forecast, {\"rmse\": rmse, \"mae\": mae, \"mape\": mape, \"rmse_cv\": rmse_cv}\n",
    "\n",
    "# Exemplo de execução do pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Suponha que você tenha um DataFrame `df` com uma coluna `sales` para rodar o pipeline\n",
    "    df = pd.DataFrame({\n",
    "        'date': pd.date_range(start='1/1/2015', periods=1000),\n",
    "        'sales': np.random.randint(0, 100, size=1000)\n",
    "    }).set_index('date')\n",
    "\n",
    "    forecast, metrics = run_forecast_pipeline(df, 'sales')\n",
    "    print(\"Previsão para os próximos 28 dias:\", forecast)\n",
    "    print(\"Métricas de erro:\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
