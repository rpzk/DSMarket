{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsão de Estoque para 28 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rpiaz\\Desenvolvimento\\DSMarket\\.venv\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Google Cloud Platform\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# Caminho para o arquivo de chave da conta de serviço\n",
    "service_account_path = 'tfm-sa.json'\n",
    "\n",
    "# Criar objeto de credenciais\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "\n",
    "# Configurações do projeto e dataset\n",
    "project_id = 'perseverance-332400'\n",
    "dataset_id = 'TFM'\n",
    "table_id = 'ds_market'\n",
    "full_table_id = f'{project_id}.{dataset_id}.{table_id}'\n",
    "\n",
    "# Função para configurar o cliente BigQuery usando credenciais específicas\n",
    "def initialize_bigquery_client():\n",
    "    return bigquery.Client(project=project_id, credentials=credentials)\n",
    "\n",
    "# Função para extrair dados históricos do BigQuery\n",
    "def get_historical_data_from_bigquery(query: str, client) -> pd.DataFrame:\n",
    "    query_job = client.query(query)\n",
    "    data = query_job.to_dataframe()\n",
    "    print(f\"Dados extraídos do BigQuery com {data.shape[0]} linhas e {data.shape[1]} colunas.\")\n",
    "    return data\n",
    "\n",
    "# Função para preparar os dados\n",
    "def prepare_item_data(raw_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Converter coluna de data\n",
    "    raw_data['date'] = pd.to_datetime(raw_data['date'])\n",
    "    # Ordenar por data e remover duplicatas\n",
    "    cleaned_data = raw_data.drop_duplicates().sort_values('date').set_index('date')\n",
    "    # Preencher valores ausentes (exemplo: preenchimento com média)\n",
    "    cleaned_data['item'] = cleaned_data['item'].fillna(cleaned_data['item'].mean())\n",
    "    print(\"Dados preparados e limpos.\")\n",
    "    return cleaned_data\n",
    "\n",
    "# Função para treinar o modelo ARIMA\n",
    "def train_forecast_model(data: pd.DataFrame, order=(1, 1, 1)):\n",
    "    print(\"Treinando o modelo ARIMA...\")\n",
    "    model = ARIMA(data['item'], order=order)\n",
    "    model_fit = model.fit()\n",
    "    print(\"Modelo treinado com sucesso.\")\n",
    "    return model_fit\n",
    "\n",
    "# Função para gerar previsões\n",
    "def predict_item(data: pd.DataFrame, model, steps=30) -> pd.DataFrame:\n",
    "    print(f\"Gerando previsões para {steps} dias...\")\n",
    "    forecast = model.get_forecast(steps=steps)\n",
    "    forecast_df = forecast.summary_frame()\n",
    "    # Criar uma coluna de datas para as previsões\n",
    "    forecast_df['date'] = pd.date_range(data.index[-1] + timedelta(days=1), periods=steps)\n",
    "    forecast_df = forecast_df.reset_index(drop=True)\n",
    "    forecast_df = forecast_df[['date', 'mean']]\n",
    "    forecast_df.columns = ['date', 'forecast_item']\n",
    "    print(\"Previsão gerada com sucesso.\")\n",
    "    return forecast_df\n",
    "\n",
    "# Função para carregar previsões no BigQuery\n",
    "def store_forecast_results(data: pd.DataFrame, table_id: str, client):\n",
    "    print(f\"Carregando previsões para a tabela {table_id} no BigQuery...\")\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "    job = client.load_table_from_dataframe(data, table_id, job_config=job_config)\n",
    "    job.result()  # Espera o job terminar\n",
    "    print(\"Previsões carregadas com sucesso no BigQuery.\")\n",
    "\n",
    "# Pipeline completo para previsão de estoque\n",
    "def run_forecast_pipeline(query: str, table_id: str, steps=30, order=(1, 1, 1)):\n",
    "    client = initialize_bigquery_client()\n",
    "    \n",
    "    # 1. Extrair dados históricos do BigQuery\n",
    "    raw_data = get_historical_data_from_bigquery(query, client)\n",
    "    \n",
    "    # 2. Preparar dados para modelagem\n",
    "    prepared_data = prepare_item_data(raw_data)\n",
    "    \n",
    "    # 3. Treinar o modelo de previsão e fazer previsões\n",
    "    model = train_forecast_model(prepared_data, order=order)\n",
    "    forecast = predict_item(prepared_data, model, steps=steps)\n",
    "    \n",
    "    # 4. Carregar previsões no BigQuery\n",
    "    store_forecast_results(forecast, table_id, client)\n",
    "\n",
    "# Definir a consulta SQL para carregar os dados\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{full_table_id}`\n",
    "\"\"\"\n",
    "\n",
    "run_forecast_pipeline(query, table_id, steps=30, order=(1, 1, 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
