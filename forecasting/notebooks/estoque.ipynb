{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsão de Estoque para 28 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Configurar logging com nível DEBUG para maior detalhamento\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Caminho para o arquivo de chave da conta de serviço\n",
    "service_account_path = 'tfm-sa.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "project_id = 'perseverance-332400'\n",
    "dataset_id = 'TFM'\n",
    "\n",
    "# Definir IDs de tabela separadas\n",
    "data_table_id = 'ds_market'               # Tabela com dados originais\n",
    "forecast_table_id = 'ds_market_forecast'  # Nova tabela para previsões\n",
    "\n",
    "full_data_table_id = f'{project_id}.{dataset_id}.{data_table_id}'\n",
    "\n",
    "# Função para configurar o cliente BigQuery usando credenciais específicas\n",
    "def initialize_bigquery_client():\n",
    "    return bigquery.Client(project=project_id, credentials=credentials)\n",
    "\n",
    "# Função para extrair dados históricos do BigQuery\n",
    "def get_historical_data_from_bigquery(query: str, client) -> pd.DataFrame:\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        data = query_job.to_dataframe()\n",
    "        logging.info(f\"Dados extraídos do BigQuery com {data.shape[0]} linhas e {data.shape[1]} colunas.\")\n",
    "        logging.info(f\"Colunas disponíveis nos dados extraídos: {data.columns.tolist()}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao extrair dados do BigQuery: {e}\")\n",
    "        return pd.DataFrame()  # Retorna um DataFrame vazio para evitar falhas subsequentes\n",
    "\n",
    "# Função para preparar os dados\n",
    "def prepare_sales_data(raw_data: pd.DataFrame, apply_log=False, aggregate_weekly=False) -> pd.DataFrame:\n",
    "    raw_data = raw_data.copy()\n",
    "    raw_data['date'] = pd.to_datetime(raw_data['date']).dt.tz_localize(None)  # Remove timezone\n",
    "    logging.debug(f\"Timezones após remoção: {raw_data['date'].dt.tz}\")  # Deve ser None\n",
    "    raw_data['sales'] = raw_data['sales'].fillna(0)\n",
    "    \n",
    "    if aggregate_weekly:\n",
    "        # Agregação semanal\n",
    "        sales_data = raw_data.groupby('date').agg({'sales': 'sum'}).resample('W').sum().sort_index()\n",
    "        logging.info(\"Dados agregados semanalmente.\")\n",
    "    else:\n",
    "        # Mantém dados diários\n",
    "        sales_data = raw_data.groupby('date').agg({'sales': 'sum'}).sort_index()\n",
    "    \n",
    "    # Preencher lacunas diárias\n",
    "    sales_data = sales_data.asfreq('D').fillna(0)\n",
    "    logging.info(\"Dados de vendas preparados e limpos.\")\n",
    "    \n",
    "    if apply_log:\n",
    "        # Aplicar transformação logarítmica\n",
    "        sales_data['sales_log'] = np.log1p(sales_data['sales'])  # log(1 + x)\n",
    "        logging.debug(\"Transformação logarítmica aplicada.\")\n",
    "    \n",
    "    return sales_data\n",
    "\n",
    "# Função para detectar outliers usando o IQR\n",
    "def detect_outliers(data: pd.DataFrame, column='sales', threshold=1.5):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Função para visualizar a série temporal\n",
    "def visualize_series(data: pd.DataFrame, store: str, item: str, save=False):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data.index, data['sales'], label='Vendas')\n",
    "    plt.title(f'Série Temporal - Loja {store} Item {item}')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Vendas')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'serie_temporal_{store}_{item}.png')\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # Plotar resíduos se as colunas existirem\n",
    "    if 'actual_sales' in data.columns and 'forecast_sales' in data.columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        residuals = data['actual_sales'] - data['forecast_sales']\n",
    "        plt.plot(data.index, residuals, label='Resíduos')\n",
    "        plt.hlines(0, data.index.min(), data.index.max(), colors='r', linestyles='dashed')\n",
    "        plt.legend()\n",
    "        plt.title(f'Residuals - Loja {store} Item {item}')\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Resíduo')\n",
    "        if save:\n",
    "            plt.savefig(f'residuals_{store}_{item}.png')\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "# Função para filtrar itens com muitas vendas zero\n",
    "def filter_zero_sales(data: pd.DataFrame, threshold=0.9) -> bool:\n",
    "    zero_sales_ratio = (data['sales'] == 0).mean()\n",
    "    logging.debug(f\"Proporção de vendas zero: {zero_sales_ratio*100:.2f}%\")\n",
    "    if zero_sales_ratio > threshold:\n",
    "        logging.warning(f\"Item com {zero_sales_ratio*100:.2f}% de vendas zero. Considerar remoção ou tratamento especial.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Função para ajustar o modelo usando auto_arima\n",
    "def fit_auto_arima_model(train_series: pd.Series):\n",
    "    # Verificar estacionariedade\n",
    "    result = adfuller(train_series)\n",
    "    logging.info(f\"ADF Statistic: {result[0]}, p-value: {result[1]}\")\n",
    "    d = 0\n",
    "    if result[1] > 0.05:\n",
    "        logging.info(\"Série não estacionária. Aplicando diferenciação.\")\n",
    "        d = 1\n",
    "        train_series = train_series.diff().dropna()\n",
    "    else:\n",
    "        logging.info(\"Série já é estacionária.\")\n",
    "    \n",
    "    model = auto_arima(\n",
    "        train_series,\n",
    "        d=d,\n",
    "        seasonal=False,\n",
    "        trace=False,\n",
    "        error_action='ignore',\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True\n",
    "    )\n",
    "    logging.info(f\"Melhor modelo ARIMA: ordem {model.order} com AIC {model.aic()}\")\n",
    "    return model\n",
    "\n",
    "# Função para ajustar o modelo Prophet\n",
    "def fit_prophet_model(train_series: pd.Series):\n",
    "    train_df = train_series.reset_index().rename(columns={'date': 'ds', 'sales': 'y'})  # Usando 'sales' diretamente\n",
    "    train_df['ds'] = train_df['ds'].dt.tz_localize(None)  # Remove timezone\n",
    "    logging.debug(f\"Timezones na coluna ds: {train_df['ds'].dt.tz}\")  # Deve ser None\n",
    "    model = Prophet(\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True  # Força a inclusão da sazonalidade anual\n",
    "    )\n",
    "    model.fit(train_df)\n",
    "    logging.info(\"Modelo Prophet ajustado com sucesso.\")\n",
    "    return model\n",
    "\n",
    "# Função para prever com Prophet\n",
    "def predict_prophet_model(model, periods: int):\n",
    "    future = model.make_future_dataframe(periods=periods, freq='D')  # Especificar frequência diária\n",
    "    forecast = model.predict(future)\n",
    "    yhat = forecast['yhat'][-periods:].values\n",
    "    logging.debug(f\"Previsões geradas: {len(yhat)} para {periods} períodos.\")\n",
    "    return yhat\n",
    "\n",
    "# Função para validação cruzada em séries temporais com ARIMA\n",
    "def cross_validate_arima(data: pd.Series, model_func, splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=splits)\n",
    "    errors = []\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(data)):\n",
    "        train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "        logging.debug(f\"Fold {fold+1}: Treino {len(train)} dias, Teste {len(test)} dias\")\n",
    "        # Ajustar o modelo no conjunto de treinamento\n",
    "        model = model_func(train)\n",
    "        # Prever no conjunto de teste\n",
    "        forecast = model.predict(n_periods=len(test))\n",
    "        # Se a série foi diferenciada, acumular as previsões\n",
    "        if model.order[1] > 0:\n",
    "            forecast = np.cumsum(forecast) + train.iloc[-1]\n",
    "        # Limitar previsões a valores não negativos\n",
    "        forecast = np.maximum(forecast, 0)\n",
    "        actual = test.values\n",
    "        # Calcular o RMSE\n",
    "        rmse = mean_squared_error(actual, forecast, squared=False)\n",
    "        logging.debug(f\"Fold {fold+1}: RMSE = {rmse}\")\n",
    "        errors.append(rmse)\n",
    "    \n",
    "    avg_rmse = np.mean(errors)\n",
    "    logging.info(f\"RMSE médio com validação cruzada: {avg_rmse}\")\n",
    "    return avg_rmse\n",
    "\n",
    "# Função para calcular as métricas de erro\n",
    "def calculate_metrics(actual, forecast):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, forecast))  # Calcula RMSE explicitamente\n",
    "    mae = mean_absolute_error(actual, forecast)\n",
    "    # Evitar divisão por zero no MAPE\n",
    "    mask = actual != 0\n",
    "    if not np.any(mask):\n",
    "        mape = np.nan\n",
    "        logging.warning(\"Todos os valores reais são zero. MAPE não pode ser calculado.\")\n",
    "    else:\n",
    "        mape = mean_absolute_percentage_error(actual[mask], forecast[mask]) * 100  # MAPE em porcentagem\n",
    "    \n",
    "    logging.info(f\"RMSE: {rmse}, MAE: {mae}, MAPE: {mape}%\")\n",
    "    return rmse, mae, mape\n",
    "\n",
    "# Função para realizar validação cruzada com Prophet\n",
    "def perform_prophet_cross_validation(model: Prophet, initial: str, period: str, horizon: str):\n",
    "    df_cv = cross_validation(model, initial=initial, period=period, horizon=horizon, parallel=\"processes\")\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    logging.info(f\"Métricas de validação cruzada:\\n{df_p}\")\n",
    "    return df_p\n",
    "\n",
    "# Função para carregar previsões no BigQuery\n",
    "def store_forecast_results(data: pd.DataFrame, table_id: str, client):\n",
    "    logging.info(f\"Carregando previsões para a tabela {table_id} no BigQuery...\")\n",
    "    \n",
    "    # Remover coluna 'actual_sales' se ela não for necessária na tabela de previsões\n",
    "    if 'actual_sales' in data.columns:\n",
    "        data = data.drop(columns=['actual_sales'])\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",\n",
    "        schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]\n",
    "    )  # Evita sobrescrever a tabela original\n",
    "    \n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(data, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "        logging.info(\"Previsões carregadas com sucesso no BigQuery.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao carregar previsões no BigQuery: {e}\")\n",
    "\n",
    "# Função para plotar previsões vs. dados reais\n",
    "def plot_forecast_with_actual(train_data: pd.DataFrame, test_data: pd.DataFrame, forecast: pd.DataFrame, store: str, item: str, save=False):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_data.index, train_data['sales'], label='Dados de Treinamento')\n",
    "    plt.plot(test_data.index, test_data['sales'], label='Vendas Reais (Teste)')\n",
    "    plt.plot(forecast['date'], forecast['forecast_sales'], label='Previsão', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(f'Previsão vs. Vendas Reais - Loja {store} Item {item}')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Vendas')\n",
    "    if save:\n",
    "        plt.savefig(f'forecast_vs_actual_{store}_{item}.png')\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # Plotar resíduos\n",
    "    if 'actual_sales' in forecast.columns and 'forecast_sales' in forecast.columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        residuals = forecast['actual_sales'] - forecast['forecast_sales']\n",
    "        plt.plot(forecast['date'], residuals, label='Resíduos')\n",
    "        plt.hlines(0, forecast['date'].min(), forecast['date'].max(), colors='r', linestyles='dashed')\n",
    "        plt.legend()\n",
    "        plt.title(f'Residuals - Loja {store} Item {item}')\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Resíduo')\n",
    "        if save:\n",
    "            plt.savefig(f'residuals_{store}_{item}.png')\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "# Função para prever usando o modelo de duas etapas\n",
    "def two_step_forecast_pipeline(train_data: pd.DataFrame, test_data: pd.DataFrame, features: list):\n",
    "    # Primeira Etapa: Classificação\n",
    "    train_data['is_sale'] = train_data['sales'] > 0\n",
    "    test_data['is_sale'] = test_data['sales'] > 0\n",
    "    \n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['is_sale']\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['is_sale']\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_class = clf.predict(X_test)\n",
    "    \n",
    "    logging.info(\"Relatório de Classificação:\")\n",
    "    logging.info(classification_report(y_test, y_pred_class))\n",
    "    \n",
    "    # Segunda Etapa: Regressão\n",
    "    X_train_reg = X_train[train_data['is_sale'] == True]\n",
    "    y_train_reg = train_data[train_data['is_sale'] == True]['sales']\n",
    "    X_test_reg = X_test[test_data['is_sale'] == True]\n",
    "    y_test_reg = test_data[test_data['is_sale'] == True]['sales']\n",
    "    \n",
    "    reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    reg.fit(X_train_reg, y_train_reg)\n",
    "    y_pred_reg = reg.predict(X_test_reg)\n",
    "    \n",
    "    # Reunir previsões finais\n",
    "    final_predictions = np.zeros(len(test_data))\n",
    "    final_predictions[test_data['is_sale'] == True] = y_pred_reg\n",
    "    \n",
    "    # Calcular métricas\n",
    "    rmse = mean_squared_error(test_data['sales'], final_predictions, squared=False)\n",
    "    mae = mean_absolute_error(test_data['sales'], final_predictions)\n",
    "    mask = test_data['sales'] != 0\n",
    "    mape = mean_absolute_percentage_error(test_data['sales'][mask], final_predictions[mask]) * 100\n",
    "    \n",
    "    logging.info(f\"RMSE: {rmse}, MAE: {mae}, MAPE: {mape}%\")\n",
    "    return rmse, mae, mape\n",
    "\n",
    "# Pipeline completo para previsão de estoque dos itens por loja\n",
    "def run_forecast_pipeline_for_top_items_per_store(query: str, forecast_table_full_id: str, model_type='Prophet', test_period_days=30, top_n=3, save_csv=False, csv_path='forecast_results.csv'):\n",
    "    client = initialize_bigquery_client()\n",
    "    raw_data = get_historical_data_from_bigquery(query, client)\n",
    "\n",
    "    # Verificar se as colunas necessárias existem\n",
    "    required_columns = {'store', 'item', 'date', 'sales'}\n",
    "    if not required_columns.issubset(raw_data.columns):\n",
    "        missing = required_columns - set(raw_data.columns)\n",
    "        raise ValueError(f\"As colunas a seguir estão faltando nos dados: {missing}\")\n",
    "\n",
    "    # Obter lista de lojas únicas\n",
    "    stores = raw_data['store'].unique()\n",
    "    all_forecasts = []\n",
    "\n",
    "    for store in stores:\n",
    "        logging.info(f\"\\nProcessando loja {store}...\")\n",
    "        store_data = raw_data[raw_data['store'] == store]\n",
    "        total_sales_per_item = store_data.groupby('item')['sales'].sum().reset_index()\n",
    "        top_items = total_sales_per_item.sort_values(by='sales', ascending=False).head(top_n)['item']  # Seleciona top N itens\n",
    "\n",
    "        for item in top_items:\n",
    "            logging.info(f\"Processando item {item} na loja {store}...\")\n",
    "            item_data = store_data[store_data['item'] == item]\n",
    "            prepared_data = prepare_sales_data(item_data, apply_log=False, aggregate_weekly=False)  # Mantém dados diários\n",
    "\n",
    "            # Filtrar itens com muitas vendas zero\n",
    "            if not filter_zero_sales(prepared_data, threshold=0.9):\n",
    "                logging.warning(f\"Item {item} na loja {store} possui alta proporção de vendas zero. Aplicando modelo de duas etapas.\")\n",
    "                \n",
    "                # Engenharia de features para modelos de duas etapas\n",
    "                prepared_data['day_of_week'] = prepared_data.index.dayofweek\n",
    "                prepared_data['month'] = prepared_data.index.month\n",
    "                # Supondo que 'event' seja uma coluna binária indicando eventos promocionais\n",
    "                # Preencher valores ausentes com 0\n",
    "                event_data = raw_data[(raw_data['store'] == store) & (raw_data['item'] == item)][['date', 'event']]\n",
    "                event_data = event_data.set_index('date').reindex(prepared_data.index, fill_value=0)\n",
    "                prepared_data['event'] = event_data['event']\n",
    "\n",
    "                features = ['day_of_week', 'month', 'event']\n",
    "                \n",
    "                # Dividir em treino e teste\n",
    "                cutoff_date = prepared_data.index.max() - pd.Timedelta(days=test_period_days)\n",
    "                train_data = prepared_data[prepared_data.index <= cutoff_date]\n",
    "                test_data = prepared_data[prepared_data.index > cutoff_date]\n",
    "\n",
    "                if len(train_data) < 2 or len(test_data) < 1:\n",
    "                    logging.warning(f\"Dados insuficientes para treinamento ou teste para item {item} na loja {store}. Pulando...\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Aplicar o modelo de duas etapas\n",
    "                    rmse, mae, mape = two_step_forecast_pipeline(train_data, test_data, features)\n",
    "                    \n",
    "                    logging.info(f\"Loja {store}, Item {item} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, MAPE: {mape:.2f}%\")\n",
    "                    \n",
    "                    # Preparar DataFrame de previsão\n",
    "                    forecast_df = pd.DataFrame({\n",
    "                        'date': test_data.index,\n",
    "                        'forecast_sales': np.where(test_data['is_sale'], test_data['sales'], 0),  # Substituir com previsões reais do modelo de duas etapas\n",
    "                        'actual_sales': test_data['sales'].values,\n",
    "                        'store': store,\n",
    "                        'item': item,\n",
    "                        'rmse': rmse,\n",
    "                        'mae': mae,\n",
    "                        'mape': mape,\n",
    "                        'model': 'Two-Step'\n",
    "                    })\n",
    "                    \n",
    "                    all_forecasts.append(forecast_df)\n",
    "                    \n",
    "                    # Plotar previsões vs. vendas reais e resíduos\n",
    "                    plot_forecast_with_actual(train_data, test_data, forecast_df, store, item)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Erro ao aplicar modelo de duas etapas para item {item} na loja {store}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                continue  # Pular para o próximo item após aplicar o modelo de duas etapas\n",
    "\n",
    "            # Continuar com o fluxo normal de modelagem para itens com menos de 90% de vendas zero\n",
    "            # Detectar outliers\n",
    "            outliers = detect_outliers(prepared_data, threshold=1.5)\n",
    "            if not outliers.empty:\n",
    "                logging.warning(f\"Detectados {len(outliers)} outliers no item {item} na loja {store}. Removendo outliers.\")\n",
    "                lower_bound = prepared_data['sales'].quantile(0.25) - 1.5 * (prepared_data['sales'].quantile(0.75) - prepared_data['sales'].quantile(0.25))\n",
    "                upper_bound = prepared_data['sales'].quantile(0.75) + 1.5 * (prepared_data['sales'].quantile(0.75) - prepared_data['sales'].quantile(0.25))\n",
    "                prepared_data = prepared_data[(prepared_data['sales'] >= lower_bound) & (prepared_data['sales'] <= upper_bound)]\n",
    "\n",
    "            # Reafirmar frequência e preencher lacunas\n",
    "            prepared_data = prepared_data.asfreq('D').fillna(0)\n",
    "\n",
    "            # Verificar frequência\n",
    "            freq = pd.infer_freq(prepared_data.index)\n",
    "            logging.debug(f\"Frequência dos dados após limpeza: {freq}\")\n",
    "            if freq != 'D':\n",
    "                logging.warning(f\"Frequência inesperada: {freq}. Corrigindo para diária.\")\n",
    "                prepared_data = prepared_data.asfreq('D').fillna(0)\n",
    "\n",
    "            # Verificar se há datas faltantes após remoção de outliers\n",
    "            missing_dates = prepared_data.asfreq('D').index.difference(prepared_data.index)\n",
    "            if len(missing_dates) > 0:\n",
    "                logging.warning(f\"Datas faltantes após limpeza: {len(missing_dates)}. Preenchendo com zero.\")\n",
    "                prepared_data = prepared_data.asfreq('D').fillna(0)\n",
    "\n",
    "            # Visualizar a série temporal\n",
    "            visualize_series(prepared_data, store, item)\n",
    "\n",
    "            if len(prepared_data) < (test_period_days + 1):\n",
    "                logging.warning(f\"Dados insuficientes após limpeza para item {item} na loja {store}. Necessário pelo menos {test_period_days + 1} dias, encontrado {len(prepared_data)} dias. Pulando...\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Definir a data de corte dinâmica: últimos 'test_period_days' dias para teste\n",
    "                cutoff_date = prepared_data.index.max() - pd.Timedelta(days=test_period_days)\n",
    "                train_data = prepared_data[prepared_data.index <= cutoff_date]\n",
    "                test_data = prepared_data[prepared_data.index > cutoff_date]\n",
    "\n",
    "                # Verificação de tamanhos\n",
    "                steps = len(test_data)\n",
    "                logging.debug(f\"Treino: {len(train_data)} dias, Teste: {steps} dias\")\n",
    "\n",
    "                if len(train_data) < 2 or steps < 1:\n",
    "                    logging.warning(f\"Dados insuficientes para treinamento ou teste para item {item} na loja {store}, pulando...\")\n",
    "                    continue\n",
    "\n",
    "                if model_type == 'ARIMA':\n",
    "                    # Ajustar o modelo ARIMA usando sales (sem transformação)\n",
    "                    model = fit_auto_arima_model(train_data['sales'])\n",
    "\n",
    "                    # Validação cruzada\n",
    "                    rmse_cv = cross_validate_arima(train_data['sales'], fit_auto_arima_model, splits=5)\n",
    "\n",
    "                    if model is None:\n",
    "                        logging.error(f\"Não foi possível ajustar o modelo ARIMA para item {item} na loja {store}.\")\n",
    "                        continue\n",
    "\n",
    "                    # Prever para o período de teste\n",
    "                    forecast = model.predict(n_periods=steps)\n",
    "\n",
    "                    # Se a série foi diferenciada, acumular as previsões\n",
    "                    if model.order[1] > 0:\n",
    "                        forecast = np.cumsum(forecast) + train_data['sales'].iloc[-1]\n",
    "\n",
    "                    # Limitar previsões a valores não negativos\n",
    "                    forecast_values = np.maximum(forecast, 0)\n",
    "\n",
    "                elif model_type == 'Prophet':\n",
    "                    # Ajustar o modelo Prophet usando sales\n",
    "                    model = fit_prophet_model(train_data['sales'])\n",
    "\n",
    "                    # Realizar validação cruzada com Prophet (opcional)\n",
    "                    # df_cv = cross_validation(model, initial='365 days', period='30 days', horizon='30 days', parallel=\"processes\")\n",
    "                    # df_p = performance_metrics(df_cv)\n",
    "                    # logging.info(f\"Métricas de validação cruzada Prophet:\\n{df_p}\")\n",
    "\n",
    "                    # Prever para o período de teste\n",
    "                    forecast_values = predict_prophet_model(model, steps)\n",
    "\n",
    "                    # Limitar previsões a valores não negativos\n",
    "                    forecast_values = np.maximum(forecast_values, 0)\n",
    "\n",
    "                else:\n",
    "                    logging.error(f\"Tipo de modelo '{model_type}' não reconhecido.\")\n",
    "                    continue\n",
    "\n",
    "                actual_values = test_data['sales'].values\n",
    "\n",
    "                # Verificação de tamanhos após previsão\n",
    "                logging.debug(f\"Tamanho de forecast_values: {len(forecast_values)}\")\n",
    "                logging.debug(f\"Tamanho de actual_values: {len(actual_values)}\")\n",
    "                if len(forecast_values) != len(actual_values):\n",
    "                    raise ValueError(f\"Tamanho da previsão ({len(forecast_values)}) não corresponde ao tamanho dos valores reais ({len(actual_values)}).\")\n",
    "\n",
    "                # Calcular métricas de avaliação\n",
    "                rmse, mae, mape = calculate_metrics(actual_values, forecast_values)\n",
    "\n",
    "                logging.info(f\"Loja {store}, Item {item} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, MAPE: {mape:.2f}%\")\n",
    "\n",
    "                # Preparar DataFrame de previsão\n",
    "                forecast_df = pd.DataFrame({\n",
    "                    'date': test_data.index,\n",
    "                    'forecast_sales': forecast_values,\n",
    "                    'actual_sales': actual_values,\n",
    "                    'store': store,\n",
    "                    'item': item,\n",
    "                    'rmse': rmse,\n",
    "                    'mae': mae,\n",
    "                    'mape': mape,\n",
    "                    'rmse_cv': rmse_cv if model_type == 'ARIMA' else np.nan,\n",
    "                    'order': str(model.order) if model_type == 'ARIMA' else 'Prophet'\n",
    "                })\n",
    "\n",
    "                all_forecasts.append(forecast_df)\n",
    "\n",
    "                # Plotar previsões vs. vendas reais e resíduos\n",
    "                plot_forecast_with_actual(train_data, test_data, forecast_df, store, item)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erro ao processar item {item} na loja {store}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if all_forecasts:\n",
    "        final_forecast = pd.concat(all_forecasts, ignore_index=True)\n",
    "        \n",
    "        # Armazenar previsões na tabela de previsões no BigQuery\n",
    "        store_forecast_results(final_forecast, forecast_table_full_id, client)\n",
    "        \n",
    "        # Salvar previsões em um arquivo CSV localmente\n",
    "        if save_csv:\n",
    "            final_forecast.to_csv(csv_path, index=False)\n",
    "            logging.info(f\"Previsões salvas com sucesso no arquivo {csv_path}.\")\n",
    "    else:\n",
    "        logging.warning(\"Nenhuma previsão foi gerada.\")\n",
    "\n",
    "# Definir a consulta SQL para carregar os dados\n",
    "query = f\"\"\"\n",
    "SELECT store, item, date, sales\n",
    "FROM `{full_data_table_id}`\n",
    "\"\"\"\n",
    "\n",
    "# Executar o pipeline, especificando a tabela de previsões e o tipo de modelo ('ARIMA' ou 'Prophet')\n",
    "run_forecast_pipeline_for_top_items_per_store(\n",
    "    query,\n",
    "    f'{project_id}.{dataset_id}.{forecast_table_id}',\n",
    "    model_type='Prophet',   # Alterar para 'ARIMA' se preferir usar ARIMA\n",
    "    test_period_days=28,    # Definir o período de teste para os últimos 28 dias\n",
    "    top_n=3,                # Número de itens a serem previstos por loja\n",
    "    save_csv=True,          # Habilita o salvamento em CSV\n",
    "    csv_path='previsoes_ds_market.csv'  # Caminho e nome do arquivo CSV\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
